{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965ab4bb-fafb-4c9a-b6c9-cf69f9eb7281",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/sushma.manthena/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 238\u001b[0m\n\u001b[1;32m    236\u001b[0m yum \u001b[38;5;241m=\u001b[39m yum[yum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuisine\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(subcuisine)]\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m#clean up ingredients and create list\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m yum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean ingredients\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43myum\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mingredients\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_ingr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m yum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean ingredients\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m yum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean ingredients\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:[remove_word(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m    240\u001b[0m yum\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/yummly_clean.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 238\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    236\u001b[0m yum \u001b[38;5;241m=\u001b[39m yum[yum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuisine\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(subcuisine)]\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m#clean up ingredients and create list\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m yum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean ingredients\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m yum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mingredients\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43msplit_ingr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    239\u001b[0m yum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean ingredients\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m yum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean ingredients\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:[remove_word(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m    240\u001b[0m yum\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/yummly_clean.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36msplit_ingr\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m cleanlist\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     22\u001b[0m lst \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m cleanlist\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(wnl\u001b[38;5;241m.\u001b[39mlemmatize(word\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,item))) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m lst]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cleanlist\n",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m cleanlist\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     22\u001b[0m lst \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m cleanlist\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(wnl\u001b[38;5;241m.\u001b[39mlemmatize(word\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[^a-zA-Z]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m lst]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cleanlist\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/sushma.manthena/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "'''Clean up recipes from yummly, to produce ingredient lists and flavor lists, and normalize flavor profile using tfidf technique\n",
    "Pickle the dataframe after clean up as 'yummly_clean.pkl'\n",
    "Pickle the dataframe projected into the ingredient space in the flavor network as 'yummly_ingr.pkl'\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "#take the long string in 'ingredients', lemmatize, regex, and split into words\n",
    "def split_ingr(x):\n",
    "    wnl=WordNetLemmatizer()\n",
    "    cleanlist=[]\n",
    "    lst = x.strip('[]').split(',')\n",
    "    cleanlist=[' '.join(wnl.lemmatize(word.lower()) for word in word_tokenize(re.sub('[^a-zA-Z]',' ',item))) for item in lst]\n",
    "    return cleanlist\n",
    "\n",
    "#remove low-information words from ingredients, could use more\n",
    "def remove_word(word):\n",
    "    alist =['low fat', 'reduced fat', 'fat free', 'fatfree', 'nonfat','gluten free', 'free range',\\\n",
    "            'reduced sodium', 'salt free','sodium free', 'low sodium', 'sweetened','unsweetened','large','extra large','oz ']\n",
    "    for item in alist:\n",
    "        word = word.replace(item,'')\n",
    "    return word\n",
    "\n",
    "#match ingredients in yummly recipes to ingredients in graph; filter out those with >3 missing matches\n",
    "def cleanup_ingredients(ingr,df,col):\n",
    "\n",
    "    df_ingr = set()\n",
    "    df[col].map(lambda x: [df_ingr.add(i) for i in x])\n",
    "\n",
    "    long_ingredients = filter(lambda x: ' ' in x, ingr)\n",
    "    short_ingredients = ingr - set(long_ingredients)\n",
    "    df_dic={}\n",
    "\n",
    "    for item in long_ingredients:\n",
    "        for key in df_ingr:\n",
    "            if item in key:\n",
    "                if key not in df_dic:\n",
    "                    df_dic[key] = [item]\n",
    "                else:\n",
    "                    df_dic[key].append(item)\n",
    "\n",
    "    for item in short_ingredients:\n",
    "        for key in df_ingr:\n",
    "            if item in key.split():\n",
    "                if key not in df_dic:\n",
    "                    df_dic[key] = [item]\n",
    "                else:\n",
    "                    df_dic[key].append(item)\n",
    "\n",
    "    diff_dic = df_ingr - set(df_dic.keys())\n",
    "\n",
    "    df_dic = tweak_dic(df_dic, diff_dic)\n",
    "    diff_dic = df_ingr - set(df_dic.keys())\n",
    "    print('length of ingredients, matched ingredients, missed ingredients')\n",
    "    print(len(df_ingr), len(df_dic.keys()), len(diff_dic))\n",
    "\n",
    "    df2 = df.copy()\n",
    "    df2['len_diff'] = df2[col].apply(lambda x: count_missing(x,df_dic))\n",
    "    df2['match ingredients'] = df2[col].apply(lambda x: ingr_replace(x,df_dic))\n",
    "    df2['len_match'] = df2['match ingredients'].apply(lambda x: len(x))\n",
    "    #remove entries with less match ingredients or no matching ingr_ingredients\n",
    "    df3 = df2[(df2['len_diff']<3) & (df2['len_match']!=0)]\n",
    "    print('dataframe shape before and after filtering')\n",
    "    print(df2.shape, df3.shape)\n",
    "\n",
    "    #sort ingredients set for later matching to flavor\n",
    "    match_ingr = set()\n",
    "    df3['match ingredients'].map(lambda x: [match_ingr.add(i) for i in x])\n",
    "    sorted_ingr = sorted(list(match_ingr))\n",
    "    #create columns for each ingredient\n",
    "    df4 = df3.copy()\n",
    "    for item in sorted_ingr:\n",
    "        df4[item] = df4['match ingredients'].apply(lambda x:item in x)\n",
    "\n",
    "    df_X = df4.drop(df3.columns, axis=1)\n",
    "\n",
    "    return df4, df_X\n",
    "\n",
    "#after direct string matching, catch some spelling differences through this\n",
    "def tweak_dic(df_dic, diff_df):\n",
    "\n",
    "    alist = ['chile', 'chili','chilies','chilli','sriracha']\n",
    "    for pepper in alist:\n",
    "        for item in filter(lambda x: pepper in x, diff_df):\n",
    "            if item not in df_dic:\n",
    "                df_dic[item] = ['tabasco pepper']\n",
    "\n",
    "    for item in filter(lambda x: 'flour' in x, diff_df):\n",
    "        if item not in df_dic:\n",
    "                df_dic[item] = ['whole grain wheat flour']\n",
    "\n",
    "    for item in filter(lambda x: 'tumeric' in x, diff_df):\n",
    "        if item not in df_dic:\n",
    "                df_dic[item] = ['turmeric']\n",
    "\n",
    "    for item in filter(lambda x: 'yoghurt' in x, diff_df):\n",
    "        if item not in df_dic:\n",
    "                df_dic[item] = ['yogurt']\n",
    "\n",
    "    for item in filter(lambda x: 'sausage' in x, diff_df):\n",
    "        if item not in df_dic:\n",
    "            df_dic[item] = ['smoked sausage']\n",
    "\n",
    "    alist = ['rib','chuck','sirloin','steak']\n",
    "    for beef in alist:\n",
    "        for item in filter(lambda x: beef in x, diff_df):\n",
    "            if item not in df_dic:\n",
    "                df_dic[item] = ['beef']\n",
    "\n",
    "    for item in filter(lambda x: 'fillet' in x, diff_df):\n",
    "        if item not in df_dic:\n",
    "                df_dic[item] = ['raw fish']\n",
    "\n",
    "    for item in filter(lambda x: 'mozzarella' in x, diff_df):\n",
    "        if item not in df_dic:\n",
    "                df_dic[item] = ['mozzarella cheese']\n",
    "\n",
    "    for item in filter(lambda x: 'spinach' in x, diff_df):\n",
    "        if item not in df_dic:\n",
    "                df_dic[item] = ['dried spinach']\n",
    "\n",
    "    for item in filter(lambda x: 'curry' in x, diff_df):\n",
    "        if item not in df_dic:\n",
    "            df_dic[item] = ['coriander','turmeric','cumin','cayenne']\n",
    "\n",
    "    return df_dic\n",
    "\n",
    "#Count missing ingredients after matching; salt, sugar, water and oil are not in the flavor network and thus don't count as missing\n",
    "def count_missing(lst, df_dic):\n",
    "    cnt = 0\n",
    "    for item in lst:\n",
    "        if item in df_dic:\n",
    "            cnt+=1\n",
    "        elif 'salt' in item.split():\n",
    "            cnt+=1\n",
    "        elif 'sugar' in item.split():\n",
    "            cnt+=1\n",
    "        elif 'water' in item.split():\n",
    "            cnt+=1\n",
    "        elif 'oil' in item.split():\n",
    "            cnt+=1\n",
    "\n",
    "    return len(lst) - cnt\n",
    "\n",
    "#After making dictionary to map ingredients from yummly recipes to ingredients in the flavor network, this is to replace ingredients in recipes with ingredients in the flavor network\n",
    "def ingr_replace(lst, df_dic):\n",
    "    temp = set()\n",
    "    for item in lst:\n",
    "        if item in df_dic:\n",
    "            temp.update(df_dic[item])\n",
    "    return temp\n",
    "\n",
    "#using flavor network to project recipes from ingredient matrix to flavor matrix\n",
    "def flavor_profile(df,ingr,comp,ingr_comp):\n",
    "    sorted_ingredients = df.columns\n",
    "    underscore_ingredients=[]\n",
    "    for item in sorted_ingredients:\n",
    "        underscore_ingredients.append(item.replace(' ','_'))\n",
    "\n",
    "    print(len(underscore_ingredients), len(sorted_ingredients))\n",
    "\n",
    "    ingr_total = ingr_comp.join(ingr,how='right',on='# ingredient id')\n",
    "    ingr_total = ingr_total.join(comp,how='right',on='compound id')\n",
    "\n",
    "    ingr_pivot = pd.crosstab(ingr_total['ingredient name'],ingr_total['compound id'])\n",
    "    ingr_flavor = ingr_pivot[ingr_pivot.index.isin(underscore_ingredients)]\n",
    "\n",
    "    df_flavor = df.values.dot(ingr_flavor.values)\n",
    "    print(df.shape, df_flavor.shape)\n",
    "\n",
    "    return df_flavor\n",
    "\n",
    "#normalize flavor matrix with tfidf method\n",
    "def make_tfidf(arr):\n",
    "    '''input, numpy array with flavor counts for each recipe and compounds\n",
    "    return numpy array adjusted as tfidf\n",
    "    '''\n",
    "    arr2 = arr.copy()\n",
    "    N=arr2.shape[0]\n",
    "    l2_rows = np.sqrt(np.sum(arr2**2, axis=1)).reshape(N, 1)\n",
    "    l2_rows[l2_rows==0]=1\n",
    "    arr2_norm = arr2/l2_rows\n",
    "\n",
    "    arr2_freq = np.sum(arr2_norm>0, axis=0)\n",
    "    arr2_idf = np.log(float(N+1) / (1.0 + arr2_freq)) + 1.0\n",
    "\n",
    "    from sklearn.preprocessing import normalize\n",
    "    tfidf = np.multiply(arr2_norm, arr2_idf)\n",
    "    tfidf = normalize(tfidf, norm='l2', axis=1)\n",
    "    print(tfidf.shape)\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    yum = pd.read_pickle('data/yummly.pkl')\n",
    "    #drop duplicates\n",
    "    yum = yum.drop_duplicates(['id'], keep='first')\n",
    "    #drop low ratings\n",
    "    yum = yum[yum['rating']>2]\n",
    "    #drop dishes such as dessert and sauce\n",
    "    yum = yum[yum['course']!='[Desserts]']\n",
    "    yum = yum[yum['course']!='[Condiments and Sauces]']\n",
    "    #clean up cuisine labels\n",
    "    yum['cuisine']= yum['cuisine'].apply(lambda x: x.strip('[]'))\n",
    "\n",
    "    cuisine_dic = {'Thai, Asian': 'Thai', 'Chinese, Asian':'Chinese', 'Japanese, Asian':'Japanese',\n",
    "     'Southern & Soul Food, American': 'Southern & Soul Food',\n",
    "     'Mediterranean, Greek': 'Mediterranean',\n",
    "     'Cajun & Creole, Southern & Soul Food, American': 'Southern & Soul Food',\n",
    "     'Asian, Japanese': 'Japanese','Cajun & Creole, American': 'Cajun & Creole',\n",
    "     'Hawaiian, American': 'Hawaiian', 'Asian, Thai': 'Thai', 'American, Cuban':'Cuban',\n",
    "     'Greek, Mediterranean': 'Greek', 'Indian, Asian': 'Indian','Asian, Chinese':'Chinese',\n",
    "     'American, Kid-Friendly': 'American', 'Spanish, Portuguese':'Spanish',\n",
    "     'Mexican, Southwestern': 'Mexican', 'Southwestern, Mexican': 'Southwestern',\n",
    "     'American, Southern & Soul Food': 'Southern & Soul Food',\n",
    "     'Cajun & Creole, Southern & Soul Food': 'Southern & Soul Food',\n",
    "     'Portuguese, American':'American','American, French': 'American',\n",
    "     'American, Cajun & Creole':'American',\n",
    "     'American, Cajun & Creole, Southern & Soul Food': 'American',\n",
    "     'Irish, American':'American'\n",
    "        }\n",
    "\n",
    "    yum['cuisine'] = yum['cuisine'].apply(lambda x: cuisine_dic[x] if x in cuisine_dic else x)\n",
    "    #remove some cusines with few dishes\n",
    "    subcuisine = list(yum['cuisine'].value_counts().index[:25])\n",
    "    yum = yum[yum['cuisine'].isin(subcuisine)]\n",
    "    #clean up ingredients and create list\n",
    "    yum['clean ingredients'] = yum['ingredients'].apply(lambda x: split_ingr(x))\n",
    "    yum['clean ingredients'] = yum['clean ingredients'].apply(lambda x:[remove_word(word) for word in x])\n",
    "    yum.to_pickle('data/yummly_clean.pkl')\n",
    "\n",
    "    #make list and set for all ingredients\n",
    "    yum_lst = list(itertools.chain(*(yum['clean ingredients'].tolist())))\n",
    "    yum_ingr = set(yum_lst)\n",
    "    print(len(yum_lst), len(yum_ingr))\n",
    "\n",
    "    #load ingr and comp information for the flavor network\n",
    "    comp = pd.read_csv('data/comp_info.tsv',index_col=0,sep='\\t')\n",
    "    ingr_comp = pd.read_csv('data/ingr_comp.tsv',sep='\\t')\n",
    "    ingr = pd.read_csv('data/ingr_info.tsv',index_col=0,sep='\\t')\n",
    "    ingr['space ingredients']= ingr['ingredient name'].apply(lambda x: x.replace('_',' ') )\n",
    "    ingr_ingredients = set()\n",
    "    ingr['space ingredients'].map(lambda x: ingr_ingredients.add(x))\n",
    "    print(len(ingr_ingredients))\n",
    "    #clean up ingredients and get two dataframes\n",
    "    yum_ingr, yum_X = cleanup_ingredients(ingr_ingredients, yum, 'clean ingredients')\n",
    "    #pickle the dataframe yum_ingr and yum_X\n",
    "    yum_ingr.to_pickle('data/yummly_ingr.pkl')\n",
    "    yum_X.to_pickle('data/yummly_ingrX.pkl')\n",
    "    #get flavor profile\n",
    "    yum_flavor = flavor_profile(yum_X, ingr, comp, ingr_comp)\n",
    "    #make tfidf from flavor profile\n",
    "    yum_tfidf = make_tfidf(yum_flavor)\n",
    "    #pickle numpy array as dataframes\n",
    "    pd.DataFrame(yum_flavor).to_pickle('data/yum_flavor.pkl')\n",
    "    pd.DataFrame(yum_tfidf).to_pickle('data/yum_tfidf.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aaab43a-04c7-46ab-8771-7e7df5f2b602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6e41f-e2c6-4297-bfa0-7f4be23ef4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
